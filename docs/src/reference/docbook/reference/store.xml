<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" version="5.0"  xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" 
  xml:id="store">

  <title>Writing and reading data using the Hadoop File System</title>

  <para>The Store sub-project of Spring for Apache Hadoop provides abstractions for
  writing and reading various types of data residing in HDFS. We currently support
  different files types either via our own store accessors or using the Kite SDK.
  Currently the store sub-project doesn’t have any xml namespace or javaconfig
  based configuration as these classes are considered to be a foundational library.
  However, this may change in future releases.</para>

  <section id="store:abstractions">
    <title>Store Abstraction</title>
    <para>Native store abstraction provides a various writer and reader interfaces
    so that user is fairly unaware of what is the actual implementation working
    against files in HDFS. Implementations are usually strongly typed and provides
    constructors and setters for additional setup to work with naming, compression
    codecs and everything else defining the behaviour. Interfaces are meant to be
    used from an integration components which doesn’t need to know internals
    of writers and readers.</para>

    <section id="store:abstractionswritingdata">
      <title>Writing Data</title>
      <para>Main interface writing into a store is a <interfacename>DataWriter</interfacename>
      which have one method write which simply writes an entity and backing
      implementation will handle the rest.</para>
      <programlisting language="java"><![CDATA[public interface DataWriter<T> {
  void write(T entity) throws IOException;
}]]></programlisting>

      <para><interfacename>DataStoreWriter</interfacename> is adding methods to close and
      flush a writer. Some of the writers have a property to close a stream after
      an idle time has been reached but generally this interface is meant for
      programmatic control of these operations.</para>
      <programlisting language="java"><![CDATA[public interface DataStoreWriter<T> extends DataWriter<T>, Flushable, Closeable {
}]]></programlisting>

      <section id="store:abstractionsfilenaming">
        <title>File Naming</title>
        <para>Different file naming strategies are used to automatically determine name
        of a file to be used. Writers without additional naming configuration will usually
        use a given base path as is. As soon as any type of a strategy is configured, given
        base path is considered to be a base directory for writer and name of the file is
        resolved by file naming strategies.</para>

        <para>For example if defined base path is <literal>“/tmp/path”</literal> and
        <classname>StaticFileNamingStrategy</classname> with <literal>“data”</literal>,
        actual file path resolved would be <literal>“/tmp/path/data”</literal>.</para>

        <programlisting language="java"><![CDATA[Path path = new Path("/tmp/path");
Configuration config = new Configuration();
TextFileWriter writer = new TextFileWriter(config, path, null);
StaticFileNamingStrategy fileNamingStrategy = new StaticFileNamingStrategy("data")
writer.setFileNamingStrategy(fileNamingStrategy);]]></programlisting>

        <para>At first look this may feel a little complicated but will make sense after
        more file naming strategies are added. These will also provide facilities for
        parallel writers or simple re-launched writer to be able to continue from next
        to be expected file name. For example, <classname>RollingFileNamingStrategy</classname>
        will add a simple increasing value to a file name and will try to initialize
        itself with a correct position.</para>

        <para>Built-in strategies currently supported are
        <classname>StaticFileNamingStrategy</classname>,
        <classname>RollingFileNamingStrategy</classname>,
        <classname>UuidFileNamingStrategy</classname> and
        <classname>CodecFileNamingStrategy</classname>.
        <classname>ChainedFileNamingStrategy</classname> can be used to chain
        multiple strategies together where each individual strategy will
        provide its own part.</para>
      </section>

      <section id="store:abstractionsfilerollover">
        <title>File Rollover</title>
        <para>File rolling strategy is used to determine a condition in a writer
        when a current stream should be automatically closed and next file to be opened.
        This is usually done together with <classname>RollingFileNamingStrategy</classname>
        to rollover when certain file size limit has been reached.</para>
        <para>Currently only one strategy <classname>SizeRolloverStrategy</classname>
        is supported.</para>
      </section>

      <section id="store:abstractionspartitioning">
        <title>Partitioning</title>
        <para>Partitioning is a concept of choosing a target file on demand either based on
        content to be written or any other information available to a writer at a time
        of a write operation. While it would be perfectly alright to use multiple writers
        manually, framework already does all the heavy lifting around partitioning. We work
        through interfaces and provide a generic default implementation still allowing to
        plug a customized version if there’s a need for it.</para>

        <para><interfacename>PartitionStrategy</interfacename> is a strategy interface
        defining <interfacename>PartitionResolver</interfacename> and
        <interfacename>PartitionKeyResolver</interfacename>.</para>

        <programlisting language="java"><![CDATA[public interface PartitionStrategy<T,K> {
  PartitionResolver<K> getPartitionResolver();
  PartitionKeyResolver<T, K> getPartitionKeyResolver();
}]]></programlisting>

        <para><interfacename>PartitionResolver</interfacename> is an interface knowing
        how to resolve arbitrary partition key into a Path. We don’t force any specific
        partition key type in an interface level itself but usually the implementation
        needs to be aware of its type.</para>

        <programlisting language="java"><![CDATA[public interface PartitionResolver<K> {
  Path resolvePath(K partitionKey);
}]]></programlisting>

        <para><interfacename>PartitionKeyResolver</interfacename> in an interface which knows
        how to create a partition key from an entity. This is needed because writer interfaces
        allow to write entities without an explicit partition key.</para>

        <programlisting language="java"><![CDATA[public interface PartitionKeyResolver<T, K> {
  K resolvePartitionKey(T entity);
}]]></programlisting>

        <para><interfacename>PartitionDataStoreWriter</interfacename> is an extension of
        <interfacename>DataStoreWriter</interfacename> adding method to write an entity
        with a partition key. In this context the partition key is something what the
        partition strategy is able to use.</para>

        <programlisting language="java"><![CDATA[public interface PartitionDataStoreWriter<T,K> extends DataStoreWriter<T> {
  void write(T entity, K partitionKey) throws IOException;
}]]></programlisting>

        <section>
          <title>DefaultPartitionStrategy</title>

          <para><classname>DefaultPartitionStrategy</classname> is a generic default
          implementation meant to be used together with a Spring SpEL expression language.
          <interfacename>PartitionResolver</interfacename> used in
          <classname>DefaultPartitionStrategy</classname> expects partition key to be a type
          of Map&#60;String,Object&#62; and partition key created by
          <interfacename>PartitionKeyResolver</interfacename> is a
          <classname>DefaultPartitionKey</classname> which itself is a
          Map&#60;String,Object&#62;.</para>

          <para>Order to make it easy to work with SpEL and partitioning, map values can be
          directly accessed with keys and additional partitioning methods has
          been registered.</para>

          <section>
            <title>Partition Path Expression</title>
            <para>SpEL expression is evaluated against a partition key passed into
            a HDFS writer.</para>

            <section>
              <title>Accessing Properties</title>
              <para>If partition key is a type of Map any property given to a
              SpEL expression is automatically resolved from a map.</para>
            </section>

            <section>
              <title>Custom Methods</title>
              <para>Addition to a normal SpEL functionality, few custom methods has been
              added to make it easier to build partition paths. These custom methods
              can be used to work with a normal partition concepts like date formatting,
              lists, ranges and hashes.</para>

              <section>
                <title>path</title>
                <programlisting language="java"><![CDATA[path(String... paths)]]></programlisting>

                <para>Concatenates paths together with a delimiter <emphasis>/</emphasis>.
                This method can be used to make the expression less verbose than using a
                native SpEL functionality to combine path parts together. To create a path
                <emphasis>part1/part2</emphasis>, expression
                <emphasis>'part1' + '/' + 'part2'</emphasis> is equivalent to
                <emphasis>path('part1','part2')</emphasis>.</para>

                <section>
                  <title>Parameters</title>
                  <formalpara>
                    <title>paths</title>
                    <para>Any number of path parts</para>
                  </formalpara>
                </section>
                <section>
                  <title>Return Value</title>
                  <para>Concatenated value of paths delimited with /.</para>
                </section>

              </section>

              <section>
                <title>dateFormat</title>
                <programlisting language="java"><![CDATA[dateFormat(String pattern)
dateFormat(String pattern, Long epoch)
dateFormat(String pattern, Date date)
dateFormat(String pattern, String datestring)
dateFormat(String pattern, String datestring, String dateformat)]]></programlisting>

                <para>Creates a path using date formatting. Internally this method delegates
                into <classname>SimpleDateFormat</classname> and needs a Date and a pattern.</para>

                <para>Method signature with three parameters can be used to create a custom
                <classname>Date</classname> object which is then passed to
                <classname>SimpleDateFormat</classname> conversion using a dateformat
                pattern. This is useful in use cases where partition should be based on
                a date or time string found from a payload content itself. Default dateformat
                pattern if omitted is <emphasis>yyyy-MM-dd</emphasis>.</para>
                <section>
                  <title>Parameters</title>
                  <formalpara>
                    <title>pattern</title>
                    <para>Pattern compatible with SimpleDateFormat to produce a final output.</para>
                  </formalpara>
                  <formalpara>
                    <title>epoch</title>
                    <para>Timestamp as Long which is converted into a Date.</para>
                  </formalpara>
                  <formalpara>
                    <title>date</title>
                    <para>A Date to be formatted.</para>
                  </formalpara>
                  <formalpara>
                    <title>dateformat</title>
                    <para>Secondary pattern to convert datestring into a Date.</para>
                  </formalpara>
                  <formalpara>
                    <title>datestring</title>
                    <para>Date as a String</para>
                  </formalpara>
                </section>
                <section>
                  <title>Return Value</title>
                  <para>A path part representation which can be a simple file or directory name or a directory structure.</para>
                </section>

              </section>

              <section>
                <title>list</title>
                <programlisting language="java"><![CDATA[
list(Object source, List<List<Object>> lists)
]]></programlisting>
                <para>Creates a partition path part by matching a source against a lists
                denoted by <emphasis>lists</emphasis>.</para>

                <para>Lets assume that data is being written and it’s possible to extrace
                an appid from a content. We can automatically do a list based
                partition by using a partition method
                <emphasis>list(appid,{{'1TO3','APP1','APP2','APP3'},{'4TO6','APP4','APP5','APP6'}})</emphasis>.
                This method would create three partitions, <emphasis>1TO3_list</emphasis>,
                <emphasis>4TO6_list</emphasis> and <emphasis>list</emphasis>.
                Latter is used if no match is found from partition lists passed to lists.</para>

                <section>
                  <title>Parameters</title>
                  <formalpara>
                    <title>source</title>
                    <para>An Object to be matched against lists.</para>
                  </formalpara>
                  <formalpara>
                    <title>lists</title>
                    <para>A definition of list of lists.</para>
                  </formalpara>
                </section>
                <section>
                  <title>Return Value</title>
                  <para>A path part prefixed with a matched key i.e. <emphasis>XXX_list</emphasis>
                  or list if no match.</para>
                </section>

              </section>

              <section>
                <title>range</title>
                <programlisting language="java"><![CDATA[
range(Object source, List<Object> list)
]]></programlisting>
                <para>Creates a partition path part by matching a source against a list denoted by
                <emphasis>list</emphasis> using a simple binary search.</para>

                <para>The partition method takes a source as first argument and list as a
                second argument. Behind the scenes this is using jvm’s binarySearch which
                works on an Object level so we can pass in anything. Remember that meaningful
                range match only works if passed in Object and types in list are of same type
                like <classname>Integer</classname>. Range is defined by a binarySearch itself
                so mostly it is to match against an upper bound except the last range in a list.
                Having a list of <emphasis>{1000,3000,5000}</emphasis> means that everything
                above 3000 will be matched with 5000. If that is an issue then simply adding
                <emphasis>Integer.MAX_VALUE</emphasis> as last range would overflow everything
                above 5000 into a new partition. Created partitions would then be
                <emphasis>1000_range</emphasis>, <emphasis>3000_range</emphasis> and
                <emphasis>5000_range</emphasis>.</para>

                <section>
                  <title>Parameters</title>
                  <formalpara>
                    <title>source</title>
                    <para>An Object to be matched against list.</para>
                  </formalpara>
                  <formalpara>
                    <title>list</title>
                    <para>A definition of list.</para>
                  </formalpara>
                </section>
                <section>
                  <title>Return Value</title>
                  <para>A path part prefixed with a matched key i.e. XXX_range.</para>
                </section>

              </section>

              <section>
                <title>hash</title>
                <programlisting language="java"><![CDATA[
hash(Object source, int bucketcount)
]]></programlisting>
                <para>Creates a partition path part by calculating hashkey using source`s
                hashCode and bucketcount. Using a partition method
                <emphasis>hash(timestamp,2)</emphasis> would then create partitions named
                <emphasis>0_hash</emphasis>, <emphasis>1_hash</emphasis> and
                <emphasis>2_hash</emphasis>. Number suffixed with _hash is simply
                calculated using <emphasis>Object.hashCode() % bucketcount</emphasis>.</para>

                <section>
                  <title>Parameters</title>
                  <formalpara>
                    <title>source</title>
                    <para>An Object which hashCode will be used.</para>
                  </formalpara>
                  <formalpara>
                    <title>bucketcount</title>
                    <para>A number of buckets</para>
                  </formalpara>
                </section>
                <section>
                  <title>Return Value</title>
                  <para>A path part prefixed with a hash key i.e. <emphasis>XXX_hash</emphasis>.</para>
                </section>

              </section>

            </section>

          </section>


        </section>

        <section>
          <title>Creating a Custom Partition Strategy</title>
          <para>Creating a custom partition strategy is as easy as just implementing needed
          interfaces. Custom strategy may be needed in use cases where it is just not feasible
          to use SpEL expressions. This will then give total flexibility to implement
          partitioning as needed.</para>

          <para>Below sample demonstrates how a simple customer id could be used as a base
          for partitioning.</para>

          <programlisting language="java"><![CDATA[public class CustomerPartitionStrategy implements PartitionStrategy<String, String> {

  CustomerPartitionResolver partitionResolver = new CustomerPartitionResolver();
  CustomerPartitionKeyResolver keyResolver = new CustomerPartitionKeyResolver();

  @Override
  public PartitionResolver<String> getPartitionResolver() {
    return partitionResolver;
  }

  @Override
  public PartitionKeyResolver<String, String> getPartitionKeyResolver() {
    return keyResolver;
  }
}

public class CustomerPartitionResolver implements PartitionResolver<String> {

  @Override
  public Path resolvePath(String partitionKey) {
    return new Path(partitionKey);
  }
}

public class CustomerPartitionKeyResolver implements PartitionKeyResolver<String, String> {

  @Override
  public String resolvePartitionKey(String entity) {
    if (entity.startsWith("customer1")) {
      return "customer1";
    } else if (entity.startsWith("customer2")) {
      return "customer2";
    } else if (entity.startsWith("customer3")) {
      return "customer3";
    }
    return null;
  }
}]]></programlisting>
        </section>


      </section>

      <section>
        <title>TextFileWriter</title>
        <para><classname>TextFileWriter</classname> is an implementation meant to write a
        simple text data where entities are separated by a delimiter. Simple example for this
        is a text file with line terminations.</para>
      </section>

      <section>
        <title>DelimitedTextFileWriter</title>
        <para><classname>DelimitedTextFileWriter</classname> is an extension atop of 
        <classname>TextFileWriter</classname> where written entity itself is also delimited.
        Simple example for this is a csv file.</para>
      </section>

      <section>
        <title>TextSequenceFileWriter</title>
        <para><classname>TextSequenceFileWriter</classname> is a similar implementation
        than <classname>TextFileWriter</classname> except that backing file is a
        Hadoop's <classname>SequenceFile</classname>.</para>
      </section>

      <section>
        <title>PartitionTextFileWriter</title>
        <para><classname>PartitionTextFileWriter</classname> is wrapping multiple
        <classname>TextFileWriter</classname>s providing automatic partitioning functionality.</para>
      </section>


    </section>

    <section id="store:abstractionsreadingdata">
      <title>Reading Data</title>
      <para>Main interface reading from a store is a <interfacename>DataReader</interfacename>.</para>

      <programlisting language="java"><![CDATA[public interface DataReader<T> {
  T read() throws IOException;
}]]></programlisting>

      <para><interfacename>DataStoreReader</interfacename> is an extension of
      <interfacename>DataReader</interfacename> providing close method for a reader.</para>

      <programlisting language="java"><![CDATA[public interface DataStoreReader<T> extends DataReader<T>, Closeable {
}]]></programlisting>

      <section id="store:abstractionsinputsplits">
        <title>Input Splits</title>
        <para>Some of the HDFS storage and file formats can be read using an input splits
        instead of reading a whole file at once. This is a fundamental concept in Hadoop’s MapReduce
        to parallelize data processing. Instead of reading a lot of small files, which would
        be a source of a Hadoop’s “small file problem”, one large file can be used. However
        one need to remember that not all file formats support input splitting especially
        when compression is used.</para>

        <para>Support for reading input split is denoted via a Split interface which simply
        mark starting and ending positions.</para>

        <programlisting language="java"><![CDATA[public interface Split {
  long getStart();
  long getLength();
  long getEnd();
}]]></programlisting>

        <para>Interface Splitter defines an contract how Split’s are calculate
        from a given path.</para>

        <programlisting language="java"><![CDATA[public interface Splitter {
  List<Split> getSplits(Path path) throws IOException;
}]]></programlisting>

        <para>We provide few generic Splitter implementations to construct Split’s.</para>

        <para><classname>StaticLengthSplitter</classname> is used to split input file with
        a given length.</para>

        <para><classname>StaticBlockSplitter</classname> is used to split input by used HDFS
        file block size. It’s also possible to split further down the road within
        the blocks itself.</para>

        <para><classname>SlopBlockSplitter</classname> is an extension of
        <classname>StaticBlockSplitter</classname> which tries to estimate how much a split
        can overflow to a next block to taggle unnecessary overhead if last file block is
        very small compared to an actual split size.</para>
      </section>

      <section>
        <title>TextFileReader</title>
        <para><classname>TextFileReader</classname> is used to read data written by
        a <classname>TextFileWriter</classname>.</para>
      </section>

      <section>
        <title>DelimitedTextFileReader</title>
        <para><classname>DelimitedTextFileReader</classname> is used to read data writte
        by a <classname>DelimitedTextFileWriter</classname>.</para>
      </section>

      <section>
        <title>TextSequenceFileReader</title>
        <para><classname>TextSequenceFileReader</classname> is used to read data written
        by a <classname>TextSequenceFileWriter</classname>.</para>
      </section>

    </section>

    <section id="store:abstractionscodecs">
      <title>Using Codecs</title>
      <para>Supported compression codecs are denoted via an interface
      <interfacename>CodecInfo</interfacename> which simply defines if codec supports
      splitting, what is it’s fully qualified java class and what is its default
      file suffix.</para>

      <programlisting language="java"><![CDATA[public interface CodecInfo {
  boolean isSplittable();
  String getCodecClass();
  String getDefaultSuffix();
}]]></programlisting>

      <para><classname>Codecs</classname> provides an enum for easy access to
      supported codecs.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis>GZIP</emphasis> - <classname>org.apache.hadoop.io.compress.GzipCodec</classname> </para>
        </listitem>
        <listitem>
          <para><emphasis>SNAPPY</emphasis> - <classname>org.apache.hadoop.io.compress.SnappyCodec</classname></para>
        </listitem>
        <listitem>
          <para><emphasis>BZIP2</emphasis> - <classname>org.apache.hadoop.io.compress.BZip2Codec</classname></para>
        </listitem>
        <listitem>
          <para><emphasis>LZO</emphasis> - <classname>com.hadoop.compression.lzo.LzoCodec</classname> (non-splittable)</para>
        </listitem>
        <listitem>
          <para><emphasis>SLZO</emphasis> - <classname>com.hadoop.compression.lzo.LzoCodec</classname> (splittable)</para>
        </listitem>
      </itemizedlist>

    </section>

  </section>

  <section id="store:dataset">
    <title>Persisting POJO datasets using Kite SDK</title>

    <para>One common requirement is to persist a large number of POJOs in
    serialized form using HDFS. The <ulink url="http://kitesdk.org/">Kite
    SDK</ulink> project provides a Kite Data Module that provides an API for
    working with datasets stored in HDFS. We are using this functionality and
    provide a some simple helper classes to aid in configuration and use in a
    Spring environment.</para>

    <section id="store:dataset:formats">
      <title>Data Formats</title>

      <para>The Kite SDK project provides support for writing data using both
      the <ulink url="http://avro.apache.org/">Avro</ulink> and <ulink
      url="http://parquet.io/">Parquet</ulink> data formats. The data format
      you choose to use influences the data types you can use in your POJO
      classes. We'll discuss the basics of the Java type mapping for the two data formats
      but we recommend that you consult each project's documentation for additional 
      details.</para>

      <note>
        <para>Currently, you can't provide your own schema. This is something that 
        we are considering changing in upcomming releases. We are also planning to provide better 
        mapping support in line with the support we currently provide for NoSQL stores like 
        MongoDB.</para>
      </note>


      <section id="store:dataset:avro">
        <title>Using Avro</title>
        <para>When using Avro as the data format the schema generation is based on reflection 
        of thet POJO class used. Primitive data types and their corresponding wrapper classes 
        are mapped to the corresponding Avro data type. More complex types, as well as the POJO 
        itself, are mapped to a record type consisting of one or more fields.</para>

        <para>The table below shows the mapping from some common types:</para>
    
        <table id="store:avro:types" pgwide="1" align="center">
          <title>Some common Java to Avro data types mapping</title>
          
          <tgroup cols="3">
            <colspec colname="c1" colwidth="1*"/>
            <colspec colname="c2" colwidth="1*"/>
            <colspec colname="c3" colwidth="1*"/>
            
            <thead>
              <row>
                <entry>Java type</entry>
                <entry>Avro type</entry>
                <entry>Comment</entry>
              </row>
            </thead>
            <tbody>
              <row>
                 <entry>String</entry>
                 <entry>string</entry>
                 <entry></entry>
              </row>
                <row>
                  <entry>int / Integer</entry>
                  <entry>int</entry>
                  <entry>32-bit signed integer</entry>
                </row>
                <row>
                  <entry>long / Long</entry>
                  <entry>long</entry>
                  <entry>64-bit signed integer</entry>
                </row>
                <row>
                  <entry>float / Float</entry>
                  <entry>float</entry>
                  <entry>32-bit floating point</entry>
                </row>
                <row>
                  <entry>double / Double</entry>
                  <entry>double</entry>
                  <entry>64-bit floating point</entry>
                </row>
                <row>
                  <entry>boolean / Boolean</entry>
                  <entry>boolean</entry>
                  <entry></entry>
                </row>
                <row>
                  <entry>byte[]</entry>
                  <entry>bytes</entry>
                  <entry>byte array</entry>
                </row>
                <row>
                  <entry>java.util.Date</entry>
                  <entry>record</entry>
                  <entry></entry>
                </row>
            </tbody>
          </tgroup>   
        </table>
  
      </section>
      <section id="store:dataset:parquet">
        <title>Using Parquet</title>
        <para>When using Parquet as the data format the schema generation is based on reflection 
        of thet POJO class used. The POJO class must be a proper JavaBean and not have any nested types. We only support
        primitive data types and their corresponding wrapper classes plus byte arrays. We do rely on the Avro-to-Parquet 
        mapping support that the Kite SDK uses, so the schema will be generated by Avro.</para>

        <note>
          <para>The Parquet support we currently povide is considered experimental. We are planning to relax a lot of the 
            restrictions on the POJO class in upcoming releases.</para>
        </note>

        <para>The table below shows the mapping from some common types:</para>
    
        <table id="store:parquet:types" pgwide="1" align="center">
          <title>Some common Java to Parquet data types mapping</title>
          
          <tgroup cols="3">
            <colspec colname="c1" colwidth="1*"/>
            <colspec colname="c2" colwidth="1*"/>
            <colspec colname="c3" colwidth="1*"/>
            
            <thead>
              <row>
                <entry>Java type</entry>
                <entry>Parquet type</entry>
                <entry>Comment</entry>
              </row>
            </thead>
            <tbody>
              <row>
                 <entry>String</entry>
                 <entry>BINARY/UTF8</entry>
                 <entry></entry>
              </row>
                <row>
                  <entry>int / Integer</entry>
                  <entry>INT32</entry>
                  <entry>32-bit signed integer</entry>
                </row>
                <row>
                  <entry>long / Long</entry>
                  <entry>INT64</entry>
                  <entry>64-bit signed integer</entry>
                </row>
                <row>
                  <entry>float / Float</entry>
                  <entry>FLOAT</entry>
                  <entry>32-bit floating point</entry>
                </row>
                <row>
                  <entry>double / Double</entry>
                  <entry>DOUBLE</entry>
                  <entry>64-bit floating point</entry>
                </row>
                <row>
                  <entry>boolean / Boolean</entry>
                  <entry>BOOLEAN</entry>
                  <entry></entry>
                </row>
                <row>
                  <entry>byte[]</entry>
                  <entry>BINARY/BYTE_ARRAY</entry>
                  <entry>byte array</entry>
                </row>
            </tbody>
          </tgroup>   
        </table>
  
      </section>

    </section>

    <section id="store:dataset:config">
      <title>Configuring the dataset support</title>

      <para>In order to use the dataset support you need to configure the following classes:
        <itemizedlist>
          <listitem>
            <para><classname>DatasetRepositoryFactory</classname> that needs a <classname>org.apache.hadoop.conf.Configuration</classname> so we 
            know how to connect to HDFS and a base path where the data will be written.</para>
          </listitem>
          <listitem>
            <para><classname>DatasetDefinition</classname> that defines the dataset you are writing. Configuration options include the 
            POJO class that is being stored, the type of format to use (Avro or Parquet). You can also specify whether to allow null values for 
            all fields (default is <emphasis>false</emphasis>) and an optional partition strategy to use for the dataset (see below for partitioning).</para>
          </listitem>
        </itemizedlist>
      The following example shows a simple configuration class:
      </para>
      <programlisting language="java"><![CDATA[@Configuration
@ImportResource("hadoop-context.xml")
public class DatasetConfig {

  private @Autowired org.apache.hadoop.conf.Configuration hadoopConfiguration;

  @Bean
  public DatasetRepositoryFactory datasetRepositoryFactory() {
    DatasetRepositoryFactory datasetRepositoryFactory = new DatasetRepositoryFactory();
    datasetRepositoryFactory.setConf(hadoopConfiguration);
    datasetRepositoryFactory.setBasePath("/user/spring");
    return datasetRepositoryFactory;
  }

  @Bean
  public DatasetDefinition fileInfoDatasetDefinition() {
    DatasetDefinition definition = new DatasetDefinition();
    definition.setFormat(Formats.AVRO.getName());
    definition.setTargetClass(FileInfo.class);
    definition.setAllowNullValues(false);
    return definition;
  }
}
]]></programlisting>
   </section>

  <section id="store:dataset:write">
    <title>Writing datasets</title>

    <para>To write datasets to Hadoop you should use either the <classname>AvroPojoDatasetStoreWriter</classname> or 
    the <classname>ParquetDatasetStoreWriter</classname> depending on the data format you want to use.</para>

      <tip>
        <para>To mark your fields as nullable use the <classname>@Nullable</classname> annotation (<classname>org.apache.avro.reflect.Nullable</classname>).
        This will result in the schema defining your field as a <emphasis>union</emphasis> of <emphasis>null</emphasis> and your datatype.</para>
      </tip>

    <para>We are using a <classname>FileInfo</classname> POJO that we have defined to hold some information based on the files we read from our local 
    file system. The dataset will be stored in a directory that is the name of the class using lowercase, so in this case it would be <emphasis>fileinfo</emphasis>.
    This directory is placed inside the <emphasis>basePath</emphasis> specified in the configuration of the <classname>DatasetRepositoryFactory</classname>.:</para>

    <programlisting language="java"><![CDATA[package org.springframework.samples.hadoop.dataset;

import org.apache.avro.reflect.Nullable;

public class FileInfo {
  private String name;
  private @Nullable String path;
  private long size;
  private long modified;

  public FileInfo(String name, String path, long size, long modified) {
    this.name = name;
    this.path = path;
    this.size = size;
    this.modified = modified;
  }

  public FileInfo() {
  }

  public String getName() {
      return name;
  }

  public String getPath() {
      return path;
  }

  public long getSize() {
      return size;
  }

  public long getModified() {
      return modified;
  }
}

]]></programlisting>

    <para>To create a writer add the following bean definition to your configuration class:</para>

    <programlisting language="java"><![CDATA[    @Bean
    public DataStoreWriter<FileInfo> dataStoreWriter() {
        return new AvroPojoDatasetStoreWriter<FileInfo>(FileInfo.class, datasetRepositoryFactory(), fileInfoDatasetDefinition());
    }
]]></programlisting>

    <para>Next, have your class use the writer bean:</para>

    <programlisting language="java"><![CDATA[    private DataStoreWriter<FileInfo> writer;

    @Autowired
    public void setDataStoreWriter(DataStoreWriter dataStoreWriter) {
        this.writer = dataStoreWriter;
    }
]]></programlisting>

    <para>Now we can use the writer, it will be opened automatically once we start writing to it:</para>

    <programlisting language="java"><![CDATA[            FileInfo fileInfo = new FileInfo(file.getName(), file.getParent(), (int)file.length(), file.lastModified());
            writer.write(fileInfo);
]]></programlisting>

    <para>Once we are done writing we should close the writer:</para>

    <programlisting language="java"><![CDATA[        try {
            writer.close();
        } catch (IOException e) {
            throw new StoreException("Error closing FileInfo", e);
        }
]]></programlisting>

    <para>We should now have dataset containing all the <emphasis>FileInfo</emphasis> entries in a <filename>/user/spring/demo/fileinfo</filename> directory:</para>

    <programlisting><![CDATA[$ hdfs dfs -ls /user/spring/*
Found 2 items
drwxr-xr-x   - trisberg supergroup          0 2014-06-09 17:09 /user/spring/fileinfo/.metadata
-rw-r--r--   3 trisberg supergroup   13824695 2014-06-09 17:10 /user/spring/fileinfo/6876f250-010a-404a-b8c8-0ce1ee759206.avro
]]></programlisting>

    <para>The <filename>.metadata</filename> directory contains dataset information including the Avro schema:</para>

    <programlisting><![CDATA[$ hdfs dfs -cat /user/spring/fileinfo/.metadata/schema.avsc
{
  "type" : "record",
  "name" : "FileInfo",
  "namespace" : "org.springframework.samples.hadoop.dataset",
  "fields" : [ {
    "name" : "name",
    "type" : "string"
  }, {
    "name" : "path",
    "type" : [ "null", "string" ],
    "default" : null
  }, {
    "name" : "size",
    "type" : "long"
  }, {
    "name" : "modified",
    "type" : "long"
  } ]
} 
]]></programlisting>

  </section>

    <section id="store:dataset:read">
    <title>Reading datasets</title>

    <para>To read datasets to Hadoop we use the <classname>DatasetTemplate</classname> class.</para>

    <para>To create a <classname>DatasetTemplate</classname> add the following bean definition to your configuration class:</para>

    <programlisting language="java"><![CDATA[ @Bean
  public DatasetOperations datasetOperations() {
    DatasetTemplate datasetOperations = new DatasetTemplate();
    datasetOperations.setDatasetRepositoryFactory(datasetRepositoryFactory());
    return datasetOperations;
  }

]]></programlisting>

    <para>Next, have your class use the <classname>DatasetTemplate</classname>:</para>

    <programlisting language="java"><![CDATA[  private DatasetOperations datasetOperations;

  @Autowired
  public void setDatasetOperations(DatasetOperations datasetOperations) {
      this.datasetOperations = datasetOperations;
  }
]]></programlisting>

    <para>Now we can read and count the entries using a <classname>RecordCallback</classname> callback interface that gets called once per
    retrieved record:</para>

    <programlisting language="java"><![CDATA[        final AtomicLong count = new AtomicLong();
        datasetOperations.read(FileInfo.class, new RecordCallback<FileInfo>() {
            @Override
            public void doInRecord(FileInfo record) {
                count.getAndIncrement();
            }
        });
        System.out.println("File count: " + count.get());

]]></programlisting>

    </section>
  
    <section id="store:dataset:partition">
    <title>Partitioning datasets</title>

    <para>To create datasets that are partitioned on one or more data fields we use the <classname>PartitionStrategy.Builder</classname> class that <emphasis>Kite</emphasis> provides.</para>
    
    <programlisting language="java"><![CDATA[        DatasetDefinition definition = new DatasetDefinition();
        definition.setPartitionStrategy(new PartitionStrategy.Builder().year("modified").build());
]]></programlisting>

    </section>
  </section>
</chapter>
